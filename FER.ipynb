{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoDvejvsDofM9dtqQgD+bK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srilekha152/House-price-prediction-/blob/main/FER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jDi3k8M9hOPr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import joblib\n",
        "\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization, Conv2D, MaxPooling2D, concatenate\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from keras.regularizers import l1, l2\n",
        "from sklearn.metrics import confusion_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('fer2013.csv')"
      ],
      "metadata": {
        "id": "6_6nEi1yiSBP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6xzdqz76ipxd",
        "outputId": "22c07f3b-8bfe-482e-dd33-3837ee48f3ef"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-351bab1d-7682-43c3-b12b-7886878c4e5c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-351bab1d-7682-43c3-b12b-7886878c4e5c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-351bab1d-7682-43c3-b12b-7886878c4e5c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-351bab1d-7682-43c3-b12b-7886878c4e5c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f0b32dc6-2a07-4385-b6e4-a43ee86da780\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f0b32dc6-2a07-4385-b6e4-a43ee86da780')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f0b32dc6-2a07-4385-b6e4-a43ee86da780 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8877,\n  \"fields\": [\n    {\n      \"column\": \"emotion\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0,\n          2,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pixels\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8740,\n        \"samples\": [\n          \"175 106 96 107 116 112 116 119 118 113 112 101 96 102 97 95 92 95 89 85 99 93 83 99 105 109 106 112 108 98 93 96 99 94 92 98 96 94 103 96 93 105 108 111 125 126 124 97 175 103 87 103 103 108 111 123 113 106 108 117 110 104 97 100 98 98 83 84 91 88 85 95 94 90 88 100 97 94 97 98 102 104 104 101 105 106 116 111 107 114 116 113 113 121 120 106 169 107 90 98 107 109 114 109 113 120 113 116 119 110 103 103 99 92 82 85 81 83 82 88 93 99 101 108 110 111 110 103 107 107 113 107 112 116 116 122 121 125 119 116 118 115 109 94 160 106 95 99 104 112 117 111 102 112 115 107 106 105 107 105 95 91 84 88 86 84 82 87 96 98 98 99 105 113 110 106 104 108 107 105 105 111 115 123 117 118 122 123 121 108 103 100 142 101 98 99 99 107 111 113 110 109 118 122 112 109 104 100 96 86 83 82 85 79 85 90 88 91 93 92 93 98 101 101 101 102 105 104 103 102 101 105 109 115 122 125 121 116 101 99 123 94 98 103 113 114 116 111 103 106 104 101 107 98 91 96 91 76 81 78 80 80 93 95 92 92 93 94 92 88 92 95 93 96 101 99 97 100 97 102 107 114 118 117 116 111 111 98 109 101 109 111 110 104 99 99 100 100 96 100 93 98 96 88 84 72 74 79 81 84 94 92 92 92 92 99 97 96 99 97 94 93 92 96 99 100 103 101 105 102 108 102 109 108 112 106 103 101 102 98 105 103 103 104 106 101 97 91 77 73 85 85 79 75 75 77 77 77 79 84 77 82 92 99 93 95 91 94 104 105 99 97 97 101 99 100 105 101 101 105 104 112 108 106 93 89 99 103 100 99 103 105 96 86 82 76 73 72 70 82 93 94 105 102 112 115 108 107 103 97 88 85 80 79 82 85 88 91 97 100 98 99 100 103 102 99 101 109 101 106 106 112 103 99 101 98 101 106 100 92 80 79 77 72 78 88 95 113 127 137 149 149 155 157 145 148 143 146 139 127 115 102 91 81 83 82 89 83 94 102 107 107 106 110 108 103 109 104 110 116 103 99 100 102 98 80 72 77 72 63 52 59 81 105 120 132 139 143 171 180 176 169 159 170 163 168 167 169 168 154 143 131 101 84 85 77 80 89 99 111 115 114 116 118 125 111 113 121 91 103 103 78 60 63 63 55 45 49 52 58 64 85 115 130 152 173 179 182 186 170 170 187 177 176 178 181 183 170 161 162 150 131 115 99 87 96 96 88 99 113 119 120 130 131 123 121 143 101 66 51 54 68 48 41 45 52 56 61 65 73 93 114 145 166 167 163 168 170 180 195 186 178 180 177 188 185 173 164 155 146 141 124 103 80 76 86 92 94 102 111 121 130 134 117 179 157 78 70 113 128 75 55 62 70 78 77 70 57 63 83 96 113 134 142 159 158 186 204 193 177 168 152 161 178 184 179 164 143 126 103 90 88 71 82 91 91 92 89 86 91 95 96 168 163 176 126 161 161 123 109 119 137 153 161 140 120 104 97 101 127 145 147 159 168 183 201 204 187 172 153 128 118 139 148 134 108 101 115 108 103 105 104 110 129 125 121 113 115 113 104 168 175 164 131 173 176 171 159 158 139 114 95 109 108 118 121 109 135 153 155 169 176 198 207 201 205 193 176 160 132 103 82 94 111 116 128 160 166 157 156 150 152 177 188 195 195 150 115 176 196 149 141 183 184 187 173 113 81 102 84 98 96 131 169 115 110 139 152 173 186 203 210 203 201 204 181 164 151 114 102 101 133 145 171 186 213 218 207 193 184 191 193 194 199 166 126 187 204 164 153 187 188 187 167 142 162 172 160 154 158 152 145 137 136 158 163 178 188 195 198 203 203 194 185 175 144 115 149 118 103 87 114 127 97 141 198 205 200 203 203 198 205 173 113 195 215 170 172 190 192 194 187 190 185 189 195 193 186 179 167 157 167 180 176 182 189 196 200 202 204 202 187 192 170 149 149 150 144 136 141 167 136 88 99 176 200 206 207 197 204 165 84 197 212 169 176 187 197 197 198 198 200 200 200 198 193 189 180 173 176 178 183 187 195 201 204 202 203 200 196 189 180 171 171 186 193 198 202 203 217 202 164 161 188 199 200 198 203 152 58 197 207 167 173 186 198 201 205 204 204 205 203 202 198 188 184 183 186 190 190 189 195 200 203 203 204 199 198 195 187 177 185 192 201 209 211 214 208 202 202 193 186 197 202 198 203 128 32 197 205 155 169 186 204 208 211 212 206 207 206 203 195 192 193 194 200 200 193 187 193 200 202 201 204 203 209 204 198 194 185 192 198 201 209 209 209 213 207 203 202 206 205 197 203 111 32 196 215 170 167 188 207 210 214 214 212 206 202 202 197 198 204 204 207 204 194 193 204 205 198 200 203 204 210 210 204 201 199 188 195 205 204 208 210 211 216 215 215 212 205 196 193 86 32 196 213 187 162 186 206 212 210 215 217 215 210 206 204 208 211 208 211 202 191 194 210 213 203 202 203 205 211 212 211 208 201 197 193 199 209 213 215 218 222 222 217 217 206 193 165 56 42 194 213 198 158 184 201 207 212 218 219 217 216 210 212 216 211 211 212 200 193 197 210 214 204 202 208 211 214 212 210 209 211 208 210 206 206 223 226 225 223 227 223 217 202 184 126 40 45 193 214 218 184 179 196 204 213 215 218 220 217 215 215 211 208 211 206 198 190 201 214 212 206 205 211 213 219 222 217 209 214 220 223 224 224 223 228 230 227 227 224 215 198 156 87 26 64 196 220 225 200 175 192 202 208 208 214 223 221 217 212 207 204 204 199 192 195 213 218 212 205 202 206 209 214 219 213 209 213 220 229 232 235 235 221 227 229 225 219 209 174 114 42 77 174 193 217 222 193 173 188 197 203 204 213 222 220 215 206 202 200 198 197 190 213 230 224 223 216 198 197 201 200 207 209 210 218 221 230 232 232 229 225 223 224 220 212 187 131 72 116 203 217 197 211 221 199 171 187 192 198 200 211 216 215 213 202 195 195 204 199 196 207 220 224 226 217 201 202 203 199 196 200 199 215 221 229 230 229 222 219 216 213 211 193 145 84 133 221 228 225 136 188 225 191 173 186 188 194 201 209 212 211 206 197 188 195 207 190 147 183 209 212 213 215 233 225 220 214 209 214 201 204 220 223 227 226 218 218 213 207 199 161 108 72 176 220 216 220 0 70 194 167 174 183 187 196 200 207 212 211 206 195 188 206 214 178 114 146 199 197 196 197 126 135 191 195 195 183 194 202 217 220 222 223 216 216 212 201 183 140 97 64 106 194 211 215 0 82 130 127 182 182 189 196 201 208 210 208 201 190 196 212 217 197 179 184 175 179 176 167 154 172 201 220 220 187 170 191 207 209 217 220 215 212 204 184 164 135 92 64 37 52 94 94 31 140 80 115 184 181 185 194 201 208 213 208 194 192 207 211 213 210 198 189 212 200 179 197 218 197 197 208 212 216 188 185 220 229 212 216 211 203 188 184 183 157 131 75 41 34 60 35 49 114 54 113 181 181 184 191 201 209 210 206 192 202 209 203 202 199 188 196 222 195 189 207 184 187 195 199 204 205 208 187 205 235 221 211 208 193 184 195 148 109 97 63 35 50 82 56 25 89 50 103 175 180 185 187 199 208 207 195 193 203 201 193 189 186 178 199 219 182 198 199 180 186 196 196 199 200 207 199 198 232 219 207 205 184 186 175 119 93 67 56 83 111 120 63 11 66 56 88 173 177 182 182 195 204 204 184 193 195 193 193 186 187 181 200 213 180 199 194 187 190 187 196 201 198 205 205 194 227 217 206 200 176 176 163 110 88 64 51 70 80 100 174 10 44 56 62 168 173 179 181 194 207 191 175 197 187 189 194 193 200 198 208 217 186 203 205 190 185 190 198 193 193 203 207 191 226 216 203 194 166 169 156 99 83 64 46 85 148 193 235 18 35 69 38 156 173 177 180 189 204 179 171 195 194 199 204 204 200 193 214 231 201 209 218 196 199 204 199 192 190 208 200 190 229 212 198 187 159 161 153 92 74 58 74 168 204 212 233 21 18 76 36 137 176 173 179 188 204 176 179 190 195 195 180 173 174 167 189 218 210 183 197 191 183 192 198 196 205 209 186 204 229 209 188 175 152 138 151 128 111 83 113 213 206 221 239 25 11 58 57 111 175 165 179 187 200 180 179 187 177 168 162 161 142 149 165 178 185 156 154 183 190 189 193 209 212 189 189 204 212 196 179 165 141 122 108 112 98 136 201 211 221 215 195 17 18 32 76 78 171 157 175 186 197 187 178 187 175 154 144 77 53 105 172 176 169 188 172 145 132 149 184 185 194 197 197 185 190 184 172 151 130 111 91 71 147 232 231 220 221 197 191 18 26 10 84 73 143 168 172 187 192 193 183 190 193 182 151 74 68 96 109 108 112 127 121 112 93 58 103 159 190 196 191 186 192 180 161 136 116 99 77 110 222 220 222 215 181 200 232 17 23 11 60 115 93 172 160 182 187 191 179 189 192 187 179 174 147 130 124 123 138 137 132 133 111 68 87 165 196 193 185 183 180 164 147 120 102 95 76 83 187 202 192 157 160 229 238 18 24 17 16 135 95 129 160 173 183 187 175 186 194 186 177 184 188 181 180 163 154 162 156 152 153 164 175 189 194 187 185 184 172 152 133 115 100 91 61 104 187 172 152 80 194 236 238 17 27 26 7 91 161 99 149 170 178 183 175 178 188 188 178 175 185 197 199 203 198 197 197 197 192 182 184 191 190 183 179 175 161 144 131 115 92 68 90 191 198 200 182 148 229 236 229 19 24 27 8 39 160 165 114 156 175 175 181 177 182 188 181 175 179 187 193 198 203 201 197 196 182 180 190 198 190 180 170 167 160 142 126 107 85 82 177 197 192 193 200 226 234 232 224 16 24 28 14 16 68 169 155 127 169 167 185 181 183 187 184 176 174 173 180 186 187 185 185 184 184 187 196 188 178 172 171 165 150 136 128 104 77 40 135 199 191 193 222 229 229 226 188 14 18 22 20 17 14 48 156 139 145 169 179 185 181 186 184 181 170 157 152 155 164 168 175 180 187 190 184 179 177 174 170 155 144 134 120 94 53 0 33 169 195 189 206 215 217 191 57\",\n          \"152 151 151 152 151 148 147 149 150 146 143 145 145 142 141 139 135 135 136 138 144 146 147 146 145 145 147 149 147 146 151 154 154 153 158 156 180 237 246 247 246 247 245 246 246 246 246 246 157 156 153 154 154 153 150 149 149 148 146 146 146 145 145 145 140 136 136 136 141 142 143 142 141 141 145 146 145 146 147 147 147 150 155 154 163 218 247 241 245 245 245 244 244 244 245 245 156 153 154 156 156 154 150 147 150 152 151 149 149 151 150 148 146 146 147 147 146 145 142 139 137 137 138 140 141 139 143 143 141 147 151 146 143 198 243 240 245 240 242 244 244 244 244 245 156 153 154 156 155 150 147 146 149 151 150 150 150 147 142 139 142 147 151 154 152 146 139 133 131 131 131 134 136 133 139 142 139 143 145 141 138 175 231 243 246 241 243 245 246 244 244 244 156 153 151 149 147 147 147 149 149 150 149 147 143 132 122 119 122 126 131 132 129 126 124 123 124 124 123 129 131 128 133 136 135 138 136 135 139 152 216 244 245 245 243 244 246 245 244 245 150 150 146 142 141 145 149 151 149 148 143 136 125 113 104 101 102 101 99 98 98 99 102 105 109 113 116 122 127 125 130 132 131 134 130 130 136 145 204 241 238 241 242 244 245 244 244 244 147 146 144 141 141 145 148 149 145 141 132 118 105 99 96 95 95 89 84 83 89 90 90 90 94 100 107 115 123 125 128 129 128 128 125 122 127 140 179 232 241 238 243 245 245 243 242 241 143 142 142 143 142 143 143 140 135 129 118 106 98 98 99 98 95 90 87 85 82 84 87 89 92 96 100 106 116 121 124 124 120 118 112 110 110 120 142 206 245 244 240 241 242 241 241 243 141 138 142 145 143 138 135 128 119 111 102 100 101 106 109 106 102 101 100 95 85 83 83 85 90 94 95 102 113 118 120 119 115 107 98 98 95 100 110 154 221 244 239 240 241 239 240 243 138 135 141 143 139 131 122 114 106 99 96 101 106 109 116 119 118 118 117 105 102 96 84 80 81 87 94 102 113 116 119 121 112 101 93 93 87 86 92 106 170 220 238 242 241 240 240 241 137 134 137 133 126 117 109 102 98 99 103 108 109 113 112 112 111 103 99 89 85 90 85 83 80 87 99 107 113 117 121 119 106 87 86 86 85 83 85 93 126 190 231 239 241 241 240 242 138 133 131 124 117 109 105 103 99 103 109 106 104 99 82 80 78 68 64 67 67 73 79 75 75 89 102 115 119 126 129 113 99 78 83 81 91 94 90 99 120 184 232 239 240 239 239 240 136 131 129 121 114 106 100 95 93 100 103 93 82 73 66 61 62 63 55 64 70 66 74 71 72 89 104 121 131 135 136 122 90 87 87 90 102 105 107 109 125 189 236 240 240 239 239 240 130 126 124 115 106 96 90 89 90 91 83 72 64 65 73 66 54 59 63 78 83 65 71 74 80 97 113 126 142 145 146 140 84 76 76 83 92 100 118 114 136 198 237 238 238 239 239 240 129 123 119 105 92 90 90 91 89 76 64 65 64 66 86 82 63 57 68 96 86 63 65 78 89 102 118 130 144 151 154 148 99 65 67 59 63 75 94 102 143 211 238 236 236 238 238 237 130 122 114 99 86 87 86 81 75 65 64 75 81 82 96 104 93 77 87 104 104 86 78 88 98 111 127 136 141 149 154 150 129 82 72 59 58 63 72 87 159 231 238 235 235 237 237 237 129 122 115 103 96 92 87 84 84 91 101 109 110 108 110 115 115 107 111 107 99 97 92 103 112 122 133 138 141 149 156 157 148 103 61 64 69 67 69 72 167 232 236 235 235 238 239 238 130 127 121 113 111 107 103 105 113 123 127 126 121 113 105 95 92 95 97 92 95 98 98 109 125 133 143 146 140 144 157 152 148 134 79 63 76 74 58 92 203 241 234 237 235 238 239 237 134 132 131 127 122 122 121 120 123 127 130 132 133 127 122 120 118 114 113 117 110 98 103 118 131 141 146 147 141 143 149 150 152 152 122 95 89 73 55 142 238 232 233 240 237 238 237 235 137 133 137 139 135 133 131 129 128 127 128 133 138 139 135 135 134 130 127 127 115 101 117 135 137 147 149 145 142 145 142 146 146 148 138 101 104 92 118 209 240 238 235 238 236 238 237 234 134 133 136 140 143 141 137 137 135 131 127 127 129 130 132 132 132 135 130 120 110 113 133 148 144 147 148 143 139 143 141 146 149 156 154 126 110 98 170 240 232 239 235 236 233 235 237 236 135 137 137 139 144 148 146 145 143 140 136 132 130 128 130 128 126 121 113 106 117 134 143 150 148 145 140 144 142 143 142 145 149 156 154 160 135 116 184 240 237 231 234 238 233 236 239 237 137 139 139 138 143 146 145 143 145 145 142 140 139 135 129 127 122 114 114 122 138 150 144 146 148 142 136 142 139 143 146 147 148 150 141 155 152 127 177 238 238 233 235 238 235 237 239 237 136 135 136 138 141 143 142 141 142 141 136 132 133 133 131 129 131 136 142 145 146 148 140 142 138 132 130 138 130 138 141 144 146 148 153 146 151 129 149 225 236 235 232 236 236 236 236 235 131 129 131 134 135 137 138 136 135 133 130 128 126 123 126 129 134 141 144 141 142 141 137 136 125 118 123 134 132 136 135 141 143 144 154 147 144 141 140 202 237 235 231 234 234 234 235 234 129 129 130 131 129 130 133 132 131 130 131 132 129 127 130 134 138 140 141 141 143 139 131 122 110 112 126 135 142 145 140 143 143 145 150 144 127 129 136 165 221 233 235 235 234 234 235 235 129 130 130 129 125 125 128 130 130 129 129 129 130 133 135 137 139 141 141 140 139 136 126 112 101 115 130 137 143 143 137 137 136 134 134 126 116 116 129 130 178 228 238 237 235 233 235 234 127 126 125 126 124 120 121 124 125 125 125 126 129 132 133 135 137 140 139 139 139 132 119 98 93 121 136 142 139 128 121 119 112 104 102 105 105 104 126 133 137 221 240 232 232 232 233 234 125 124 123 123 124 119 117 119 120 120 121 125 127 128 131 132 133 136 136 134 132 125 113 95 94 120 130 125 112 98 93 87 78 81 91 95 90 92 123 149 125 206 238 230 233 233 233 234 125 124 124 124 123 120 118 116 117 117 119 124 127 128 131 130 131 133 133 131 131 121 101 83 84 106 115 106 81 58 51 48 53 70 85 85 86 104 128 143 131 193 234 233 233 234 235 234 126 125 123 124 123 122 119 115 115 118 120 123 125 128 129 129 128 131 130 126 123 115 99 83 77 83 88 69 45 44 56 63 69 73 74 77 89 114 131 135 134 188 233 235 233 233 234 233 125 124 124 126 125 122 118 114 115 117 119 120 123 126 127 127 127 129 129 124 120 113 98 87 77 70 69 71 68 73 80 76 75 73 71 79 86 105 121 134 131 187 233 234 232 231 231 231 125 126 127 127 125 122 119 119 120 120 120 123 124 122 122 126 128 128 128 125 119 106 98 104 104 88 81 85 86 88 88 84 77 74 78 84 90 104 117 122 122 186 232 233 232 233 232 231 125 125 127 126 123 125 124 125 126 125 123 126 126 125 127 130 132 131 127 122 113 103 103 116 122 117 116 113 106 101 97 89 82 83 89 91 92 100 110 116 117 176 230 232 232 233 233 232 127 126 126 125 123 126 126 128 129 127 125 126 128 130 131 133 134 131 124 119 110 102 110 123 129 129 128 121 114 109 106 98 90 90 98 97 92 92 100 110 113 164 227 232 231 234 233 232 127 126 126 126 124 124 125 127 127 126 126 126 130 132 132 132 131 126 119 114 106 104 115 126 127 127 124 120 117 111 109 103 97 98 105 103 93 87 94 106 110 152 224 233 231 234 234 232 127 125 124 124 123 121 123 124 124 124 126 129 131 132 130 128 125 119 114 108 101 106 119 126 125 124 122 117 115 115 113 108 104 103 107 109 99 89 92 103 107 140 222 234 230 232 233 232 126 124 123 124 123 119 121 123 124 124 127 129 129 128 126 123 119 113 106 101 100 109 119 119 117 117 117 116 118 122 119 116 113 110 109 111 105 94 92 100 102 127 221 235 230 231 232 231 125 124 124 124 124 118 120 123 124 125 127 128 125 122 121 117 113 109 101 96 98 111 115 110 107 107 111 122 128 123 110 103 107 111 110 110 107 97 91 96 98 117 220 236 230 231 231 231 123 123 124 123 120 119 119 120 120 122 122 121 121 119 116 112 109 101 93 92 101 108 108 100 99 106 114 123 114 97 82 78 83 85 84 88 108 101 88 94 97 111 217 236 229 229 230 232 123 123 123 121 120 119 118 118 118 119 119 118 119 118 113 109 103 98 94 96 102 102 96 88 100 109 102 85 75 79 80 82 85 88 86 77 97 103 95 94 88 112 215 235 233 234 234 232 120 120 118 118 118 119 120 120 119 119 119 119 119 115 111 106 101 97 95 98 98 91 87 83 85 81 71 74 75 77 74 70 70 70 77 82 90 98 90 91 85 123 222 237 232 230 231 230 117 117 117 116 116 118 121 120 119 117 116 118 117 112 109 106 103 98 94 93 89 77 68 58 54 57 70 84 87 96 100 97 91 82 83 78 79 91 91 96 87 141 221 231 230 232 233 231 114 116 119 118 116 116 118 117 116 114 114 116 116 111 108 106 105 102 95 88 77 74 74 71 80 91 98 103 104 112 115 112 115 120 127 105 81 83 90 89 78 158 222 224 225 227 229 230 113 114 117 116 114 114 114 115 115 114 114 115 114 110 105 103 105 103 94 86 74 79 95 105 110 107 106 105 99 102 104 106 109 109 115 106 84 81 92 96 85 176 228 225 223 222 222 223 114 114 114 113 112 113 112 112 111 111 111 111 108 103 102 102 103 99 89 80 88 97 110 112 108 101 100 99 93 96 99 103 106 103 106 108 89 77 89 95 88 192 234 228 231 232 228 225 114 114 112 112 112 111 109 108 107 107 106 104 102 100 99 100 98 91 83 79 93 103 107 102 100 96 86 83 83 85 83 81 79 82 87 92 81 81 95 98 96 206 238 231 232 231 231 231 113 112 112 113 113 110 109 107 106 104 103 101 100 98 94 95 92 84 81 88 99 104 101 98 97 94 85 77 76 72 67 65 65 71 75 77 79 86 90 92 121 213 234 230 228 227 230 231\",\n          \"162 165 180 160 145 139 95 85 83 74 76 72 68 50 46 47 28 25 29 31 28 32 31 26 30 38 56 70 59 50 42 41 64 59 124 180 170 188 196 195 193 193 192 162 140 172 151 134 128 129 167 155 118 75 53 41 38 56 65 46 36 34 25 31 16 23 24 32 29 22 30 31 31 42 61 63 48 37 31 40 39 31 76 124 133 151 191 194 193 193 193 161 138 177 157 142 124 107 144 155 105 51 60 80 58 59 47 32 24 26 32 25 32 31 25 25 40 30 25 34 39 50 63 48 36 32 30 31 34 41 48 40 56 80 160 197 192 193 193 165 140 162 137 137 134 116 130 143 90 65 60 60 50 31 33 23 14 18 44 28 20 19 38 38 25 41 34 29 37 40 40 20 21 27 34 52 38 36 45 48 46 52 71 173 198 192 193 165 128 133 131 149 148 152 163 147 71 73 69 48 40 29 21 16 17 25 42 30 26 32 34 44 52 45 45 42 41 51 55 46 22 29 29 43 42 32 34 35 36 40 36 97 181 193 192 179 163 168 178 186 175 182 195 132 61 81 77 42 38 34 36 31 24 31 32 38 56 69 81 94 119 108 54 53 63 60 50 32 27 28 33 34 40 26 24 33 31 29 38 49 90 186 194 191 192 193 195 197 188 189 196 117 63 93 60 27 35 32 44 37 19 23 27 61 111 124 130 135 140 137 88 74 69 45 31 23 19 23 33 41 47 55 30 22 26 28 35 46 42 132 198 194 194 192 194 195 189 189 197 120 70 63 36 33 31 37 24 39 64 72 85 110 127 134 137 142 151 152 123 98 70 39 26 21 19 17 29 36 39 45 47 38 35 28 32 41 37 104 192 194 195 194 193 195 190 191 195 157 79 49 23 40 45 26 31 91 121 132 136 129 132 144 149 151 157 158 151 106 59 50 36 21 19 19 23 39 56 51 36 42 37 36 32 33 41 90 171 197 194 195 194 196 190 191 190 188 110 40 53 45 22 30 68 118 129 130 132 136 141 148 150 153 159 155 152 140 75 51 40 32 19 24 28 42 60 47 34 38 45 42 41 42 47 51 70 160 197 192 195 197 184 190 190 187 145 54 41 27 9 39 91 127 134 132 131 138 147 148 152 160 158 155 154 159 143 64 42 38 32 22 28 46 57 35 33 39 51 56 46 43 42 38 30 61 173 197 195 196 180 190 192 188 128 87 56 31 6 60 107 125 133 136 136 140 147 152 161 159 149 143 139 148 158 102 40 34 39 35 31 37 55 45 37 43 60 64 41 35 37 32 35 33 120 200 195 197 181 189 181 157 74 58 80 29 26 81 112 121 116 121 138 148 152 157 163 134 99 94 86 94 102 113 75 34 39 40 46 54 41 38 45 43 54 49 35 39 29 34 43 45 90 191 196 197 185 182 142 85 54 35 33 25 44 92 102 80 71 84 103 131 152 152 148 109 86 92 77 68 56 66 62 36 33 47 55 61 44 37 40 24 27 43 50 37 29 27 40 39 85 191 195 196 183 177 106 58 34 29 16 29 52 83 50 38 52 84 113 127 150 157 150 149 120 114 110 116 124 130 132 104 54 33 37 47 52 39 16 18 38 61 52 34 28 28 37 36 50 159 199 195 173 167 130 71 30 21 21 31 56 56 63 111 118 108 114 146 154 164 166 173 151 145 158 160 163 171 180 182 154 103 63 68 93 95 41 17 34 50 48 38 26 38 40 38 30 102 197 196 134 129 118 94 46 17 28 33 42 97 146 154 155 140 124 131 149 163 172 170 156 148 142 132 129 132 144 158 174 169 132 116 126 121 96 25 14 22 34 43 44 45 25 31 34 71 178 199 160 119 105 115 133 100 46 30 67 145 151 139 130 122 113 113 117 153 169 160 150 141 128 109 94 95 110 138 154 163 154 134 121 115 112 50 23 33 27 42 52 43 27 31 39 50 160 200 193 187 184 180 166 125 44 29 96 153 119 83 106 124 117 112 98 144 160 150 149 148 119 94 57 81 116 98 129 150 159 154 132 122 97 46 22 41 31 26 37 44 36 34 39 41 151 198 188 189 189 180 139 63 10 27 98 125 65 62 53 123 147 128 107 137 150 140 139 151 139 142 47 89 190 124 101 144 168 173 148 130 96 48 15 39 35 25 29 42 48 31 37 44 131 184 188 189 185 140 70 25 9 25 99 85 72 99 28 138 174 113 115 140 140 138 130 146 135 128 118 135 143 132 141 151 164 178 163 141 109 45 16 33 47 15 23 31 43 42 37 42 74 115 189 190 156 129 76 36 16 28 106 107 89 105 97 128 129 106 120 143 145 141 140 132 145 144 160 171 167 169 168 164 159 161 163 150 120 52 24 38 35 22 18 33 33 36 43 47 54 61 191 184 149 154 80 47 22 31 118 125 96 122 154 141 122 95 125 150 152 141 148 142 151 159 160 168 173 171 167 161 158 159 154 148 135 83 26 42 38 19 27 35 36 40 48 59 51 53 191 184 140 141 82 57 38 47 123 132 133 148 140 131 128 97 132 153 144 137 149 153 153 159 154 156 159 161 163 165 165 161 153 142 137 109 39 49 40 15 29 41 31 34 43 56 52 56 189 189 136 129 103 101 40 54 123 133 136 135 130 141 128 107 143 149 142 129 134 141 165 165 158 158 159 164 169 174 170 158 152 138 133 120 73 83 54 29 39 26 25 26 34 46 44 46 188 184 166 116 102 92 64 59 122 135 133 130 138 148 135 120 163 165 157 144 136 128 158 174 163 166 170 169 170 171 170 159 150 139 132 114 83 95 55 53 49 21 25 24 18 31 48 55 187 176 180 129 104 77 59 52 114 131 132 139 146 162 131 104 162 171 150 136 148 143 153 162 165 174 177 175 170 164 160 154 147 141 130 102 75 85 55 63 56 30 32 41 39 35 47 50 187 172 170 146 114 82 43 49 110 125 134 146 156 174 129 93 110 131 119 99 116 129 152 154 161 172 175 169 163 159 152 153 150 143 131 99 73 91 77 54 59 51 39 48 54 40 47 51 186 171 160 154 112 98 52 45 98 119 134 150 163 170 132 108 93 105 130 137 123 131 152 159 159 170 170 167 157 152 156 155 153 143 130 99 95 144 136 113 97 58 49 41 41 32 37 49 181 173 155 150 120 83 57 51 78 111 126 151 165 157 126 107 108 132 134 137 128 135 143 154 160 167 168 164 155 152 156 159 155 144 132 107 108 168 166 161 87 18 32 38 34 36 33 44 172 169 154 149 138 83 43 49 76 100 115 140 156 140 116 101 107 129 125 138 129 125 132 143 158 161 167 161 161 159 156 157 155 148 132 109 100 126 120 76 25 8 41 41 28 31 40 67 172 168 163 163 158 109 88 37 55 103 102 134 147 127 110 110 128 140 144 147 146 144 147 150 154 152 162 158 155 157 156 158 156 151 134 108 89 105 56 23 23 18 29 29 30 27 33 44 181 178 179 176 174 119 76 30 36 112 96 118 132 125 113 117 110 96 108 97 102 115 130 148 146 147 161 160 158 157 157 158 157 150 134 109 91 107 57 17 14 14 20 28 43 58 51 38 185 185 185 179 168 123 69 52 41 99 105 105 123 137 114 87 92 95 97 105 116 114 108 120 128 138 160 161 155 154 154 154 156 145 131 105 100 104 51 36 21 18 29 35 65 101 64 40 183 185 183 172 159 148 136 133 102 94 106 102 126 138 106 84 109 128 131 138 141 142 145 157 152 147 159 154 155 154 152 155 153 143 121 98 109 108 51 49 39 26 31 40 84 90 55 49 183 182 179 170 159 152 150 148 149 127 106 103 129 135 110 110 105 110 117 119 127 140 153 162 158 156 154 155 158 152 153 150 146 137 115 99 118 110 42 28 48 32 31 29 41 52 47 41 185 180 176 170 159 151 146 146 145 132 106 104 118 131 123 132 125 121 116 117 138 154 159 156 150 147 147 155 158 156 152 142 136 128 105 107 126 109 42 9 35 34 33 32 37 41 38 51 188 181 173 165 157 151 146 146 149 108 88 106 107 119 123 123 134 123 124 129 143 148 150 153 153 150 141 148 157 156 148 137 128 111 102 121 134 112 47 4 14 33 27 26 39 44 31 52 188 183 174 164 156 151 149 147 148 94 46 101 105 102 122 120 117 114 132 136 137 133 136 147 149 143 137 144 153 149 140 127 107 98 118 139 141 115 43 5 3 13 29 26 31 28 33 44 189 186 179 170 160 152 148 146 148 112 30 62 107 93 112 113 106 120 129 135 137 135 129 140 136 137 135 138 145 136 126 102 96 121 139 148 146 120 47 19 16 13 14 6 8 18 28 46 189 188 183 174 165 153 145 144 147 103 42 16 78 95 100 107 107 126 140 147 145 148 136 132 131 125 123 130 134 119 99 102 133 148 153 154 149 128 71 32 15 36 47 33 18 16 27 55 189 188 185 178 169 158 149 144 146 114 48 22 20 81 91 106 112 120 137 143 136 130 134 133 130 114 114 114 101 88 111 147 159 157 159 158 150 136 74 19 12 32 48 44 39 24 46 68 188 187 185 180 171 161 153 147 143 144 95 58 46 45 76 105 115 111 108 116 115 117 122 121 106 99 90 85 92 120 147 162 164 159 160 158 152 137 69 22 20 7 41 48 48 55 77 98 187 186 185 180 171 161 155 150 144 141 136 97 70 62 32 64 105 109 100 92 93 97 89 82 77 78 92 112 130 142 157 166 166 164 164 156 155 121 35 28 23 12 59 71 68 90 132 135 188 187 186 182 173 163 156 151 145 137 136 121 78 58 28 16 52 59 65 67 70 66 67 71 95 120 136 141 142 150 163 170 168 167 165 159 149 57 25 31 24 20 63 69 72 74 100 143 189 188 188 185 178 167 158 151 146 143 141 128 105 58 32 25 63 63 56 68 79 90 108 128 145 152 152 156 153 158 169 169 167 167 163 165 87 25 31 30 25 26 61 60 70 73 72 88 189 189 188 187 181 170 158 151 147 146 145 137 131 94 32 37 88 107 90 90 112 132 143 157 159 160 162 160 160 161 167 169 170 168 169 116 30 33 28 32 25 33 62 61 63 67 70 75 188 188 188 188 184 173 164 158 153 149 146 145 141 128 75 33 75 104 95 82 103 133 152 164 166 163 162 161 160 159 164 167 165 171 147 43 31 31 27 33 26 35 59 60 60 61 61 62\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Usage\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Training\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = []\n",
        "y_train = []\n",
        "X_test = []\n",
        "y_test = []\n",
        "for index, row in df.iterrows():\n",
        "    k = row['pixels'].split(\" \")\n",
        "    if row['Usage'] == 'Training':\n",
        "        X_train.append(np.array(k))\n",
        "        y_train.append(row['emotion'])\n",
        "    elif row['Usage'] == 'PublicTest':\n",
        "        X_test.append(np.array(k))\n",
        "        y_test.append(row['emotion'])"
      ],
      "metadata": {
        "id": "fF43mfw9itRg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKDATgiPk3UT",
        "outputId": "92243065-affb-4ac0-c225-229b9a9a0782"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['70', '80', '82', ..., '106', '109', '82'], dtype='<U3')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(X_train, dtype = 'uint8')\n",
        "y_train = np.array(y_train, dtype = 'uint8')\n",
        "X_test = np.array(X_test, dtype = 'uint8')\n",
        "y_test = np.array(y_test, dtype = 'uint8')"
      ],
      "metadata": {
        "id": "rwO-EWjfk515"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train= to_categorical(y_train, num_classes=7)\n",
        "y_test = to_categorical(y_test, num_classes=7)"
      ],
      "metadata": {
        "id": "aFqrYcgilBeg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)"
      ],
      "metadata": {
        "id": "QmrVL4UYlFxG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "# datagen = ImageDataGenerator(\n",
        "#     rescale=1./255,\n",
        "#     rotation_range = 10,\n",
        "#     horizontal_flip = True,\n",
        "#     width_shift_range=0.1,\n",
        "#     height_shift_range=0.1,\n",
        "#     fill_mode = 'nearest')\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=10,\n",
        "    horizontal_flip=True,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    fill_mode='nearest'\n",
        ")"
      ],
      "metadata": {
        "id": "ZFvz4nX9lILu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testgen = ImageDataGenerator(rescale=1./255)\n",
        "datagen.fit(X_train)"
      ],
      "metadata": {
        "id": "E_0jy1f1lMea"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64"
      ],
      "metadata": {
        "id": "mjfSg3PPlPJR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_flow = datagen.flow(X_train, y_train, batch_size=batch_size)\n",
        "test_flow = testgen.flow(X_test, y_test, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "ediBPOQFlRF0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FER_Model(input_shape=(48,48,1)):\n",
        "    # first input model\n",
        "    visible = Input(shape=input_shape, name='input')\n",
        "    num_classes = 7\n",
        "    #the 1-st block\n",
        "    conv1_1 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_1')(visible)\n",
        "    conv1_1 = BatchNormalization()(conv1_1)\n",
        "    conv1_2 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_2')(conv1_1)\n",
        "    conv1_2 = BatchNormalization()(conv1_2)\n",
        "    pool1_1 = MaxPooling2D(pool_size=(2,2), name = 'pool1_1')(conv1_2)\n",
        "    drop1_1 = Dropout(0.3, name = 'drop1_1')(pool1_1)#the 2-nd block\n",
        "    conv2_1 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_1')(drop1_1)\n",
        "    conv2_1 = BatchNormalization()(conv2_1)\n",
        "    conv2_2 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_2')(conv2_1)\n",
        "    conv2_2 = BatchNormalization()(conv2_2)\n",
        "    conv2_3 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_3')(conv2_2)\n",
        "    conv2_2 = BatchNormalization()(conv2_3)\n",
        "    pool2_1 = MaxPooling2D(pool_size=(2,2), name = 'pool2_1')(conv2_3)\n",
        "    drop2_1 = Dropout(0.3, name = 'drop2_1')(pool2_1)#the 3-rd block\n",
        "    conv3_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_1')(drop2_1)\n",
        "    conv3_1 = BatchNormalization()(conv3_1)\n",
        "    conv3_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_2')(conv3_1)\n",
        "    conv3_2 = BatchNormalization()(conv3_2)\n",
        "    conv3_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_3')(conv3_2)\n",
        "    conv3_3 = BatchNormalization()(conv3_3)\n",
        "    conv3_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_4')(conv3_3)\n",
        "    conv3_4 = BatchNormalization()(conv3_4)\n",
        "    pool3_1 = MaxPooling2D(pool_size=(2,2), name = 'pool3_1')(conv3_4)\n",
        "    drop3_1 = Dropout(0.3, name = 'drop3_1')(pool3_1)#the 4-th block\n",
        "    conv4_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_1')(drop3_1)\n",
        "    conv4_1 = BatchNormalization()(conv4_1)\n",
        "    conv4_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_2')(conv4_1)\n",
        "    conv4_2 = BatchNormalization()(conv4_2)\n",
        "    conv4_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_3')(conv4_2)\n",
        "    conv4_3 = BatchNormalization()(conv4_3)\n",
        "    conv4_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_4')(conv4_3)\n",
        "    conv4_4 = BatchNormalization()(conv4_4)\n",
        "    pool4_1 = MaxPooling2D(pool_size=(2,2), name = 'pool4_1')(conv4_4)\n",
        "    drop4_1 = Dropout(0.3, name = 'drop4_1')(pool4_1)\n",
        "\n",
        "    #the 5-th block\n",
        "    conv5_1 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_1')(drop4_1)\n",
        "    conv5_1 = BatchNormalization()(conv5_1)\n",
        "    conv5_2 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_2')(conv5_1)\n",
        "    conv5_2 = BatchNormalization()(conv5_2)\n",
        "    conv5_3 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_3')(conv5_2)\n",
        "    conv5_3 = BatchNormalization()(conv5_3)\n",
        "    conv5_4 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_4')(conv5_3)\n",
        "    conv5_3 = BatchNormalization()(conv5_3)\n",
        "    pool5_1 = MaxPooling2D(pool_size=(2,2), name = 'pool5_1')(conv5_4)\n",
        "    drop5_1 = Dropout(0.3, name = 'drop5_1')(pool5_1)#Flatten and output\n",
        "    flatten = Flatten(name = 'flatten')(drop5_1)\n",
        "    ouput = Dense(num_classes, activation='softmax', name = 'output')(flatten)# create model\n",
        "    model = Model(inputs =visible, outputs = ouput)\n",
        "    # summary layers\n",
        "    print(model.summary())\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "AJICPoK5lWUT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FER_Model()\n",
        "opt = Adam(learning_rate=0.0001, decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y_r2Aer7lcyT",
        "outputId": "c4eef8f1-eeeb-4b84-856e-5e883b1feefe"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input (\u001b[38;5;33mInputLayer\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m1\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " conv1_1 (\u001b[38;5;33mConv2D\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m640\u001b[0m \n",
              "\n",
              " batch_normalization                   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m256\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                               \n",
              "\n",
              " conv1_2 (\u001b[38;5;33mConv2D\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)                    \u001b[38;5;34m36,928\u001b[0m \n",
              "\n",
              " batch_normalization_1                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m256\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                               \n",
              "\n",
              " pool1_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " drop1_1 (\u001b[38;5;33mDropout\u001b[0m)                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " conv2_1 (\u001b[38;5;33mConv2D\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)                   \u001b[38;5;34m73,856\u001b[0m \n",
              "\n",
              " batch_normalization_2                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m512\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                               \n",
              "\n",
              " conv2_2 (\u001b[38;5;33mConv2D\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m147,584\u001b[0m \n",
              "\n",
              " batch_normalization_3                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m512\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                               \n",
              "\n",
              " conv2_3 (\u001b[38;5;33mConv2D\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m147,584\u001b[0m \n",
              "\n",
              " pool2_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)                        \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " drop2_1 (\u001b[38;5;33mDropout\u001b[0m)                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)                        \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " conv3_1 (\u001b[38;5;33mConv2D\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m295,168\u001b[0m \n",
              "\n",
              " batch_normalization_5                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m1,024\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                               \n",
              "\n",
              " conv3_2 (\u001b[38;5;33mConv2D\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m590,080\u001b[0m \n",
              "\n",
              " batch_normalization_6                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m1,024\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                               \n",
              "\n",
              " conv3_3 (\u001b[38;5;33mConv2D\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m590,080\u001b[0m \n",
              "\n",
              " batch_normalization_7                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m1,024\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                               \n",
              "\n",
              " conv3_4 (\u001b[38;5;33mConv2D\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m590,080\u001b[0m \n",
              "\n",
              " batch_normalization_8                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m1,024\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                               \n",
              "\n",
              " pool3_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " drop3_1 (\u001b[38;5;33mDropout\u001b[0m)                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " conv4_1 (\u001b[38;5;33mConv2D\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m590,080\u001b[0m \n",
              "\n",
              " batch_normalization_9                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)                      \u001b[38;5;34m1,024\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                               \n",
              "\n",
              " conv4_2 (\u001b[38;5;33mConv2D\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m590,080\u001b[0m \n",
              "\n",
              " batch_normalization_10                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)                      \u001b[38;5;34m1,024\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                               \n",
              "\n",
              " conv4_3 (\u001b[38;5;33mConv2D\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m590,080\u001b[0m \n",
              "\n",
              " batch_normalization_11                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)                      \u001b[38;5;34m1,024\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                               \n",
              "\n",
              " conv4_4 (\u001b[38;5;33mConv2D\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m590,080\u001b[0m \n",
              "\n",
              " batch_normalization_12                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)                      \u001b[38;5;34m1,024\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                               \n",
              "\n",
              " pool4_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " drop4_1 (\u001b[38;5;33mDropout\u001b[0m)                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " conv5_1 (\u001b[38;5;33mConv2D\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)                  \u001b[38;5;34m1,180,160\u001b[0m \n",
              "\n",
              " batch_normalization_13                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)                      \u001b[38;5;34m2,048\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                               \n",
              "\n",
              " conv5_2 (\u001b[38;5;33mConv2D\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)                  \u001b[38;5;34m2,359,808\u001b[0m \n",
              "\n",
              " batch_normalization_14                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)                      \u001b[38;5;34m2,048\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                               \n",
              "\n",
              " conv5_3 (\u001b[38;5;33mConv2D\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)                  \u001b[38;5;34m2,359,808\u001b[0m \n",
              "\n",
              " batch_normalization_15                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)                      \u001b[38;5;34m2,048\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                               \n",
              "\n",
              " conv5_4 (\u001b[38;5;33mConv2D\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)                  \u001b[38;5;34m2,359,808\u001b[0m \n",
              "\n",
              " pool5_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " drop5_1 (\u001b[38;5;33mDropout\u001b[0m)                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " flatten (\u001b[38;5;33mFlatten\u001b[0m)                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                                \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " output (\u001b[38;5;33mDense\u001b[0m)                        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)                              \u001b[38;5;34m3,591\u001b[0m \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)                         </span><span style=\"font-weight: bold\"> Output Shape                </span><span style=\"font-weight: bold\">         Param # </span>\n",
              "\n",
              " input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " conv1_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> \n",
              "\n",
              " batch_normalization                   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                               \n",
              "\n",
              " conv1_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> \n",
              "\n",
              " batch_normalization_1                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                               \n",
              "\n",
              " pool1_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " drop1_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " conv2_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> \n",
              "\n",
              " batch_normalization_2                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                               \n",
              "\n",
              " conv2_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> \n",
              "\n",
              " batch_normalization_3                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                               \n",
              "\n",
              " conv2_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> \n",
              "\n",
              " pool2_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " drop2_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " conv3_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> \n",
              "\n",
              " batch_normalization_5                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                               \n",
              "\n",
              " conv3_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> \n",
              "\n",
              " batch_normalization_6                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                               \n",
              "\n",
              " conv3_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> \n",
              "\n",
              " batch_normalization_7                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                               \n",
              "\n",
              " conv3_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> \n",
              "\n",
              " batch_normalization_8                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                               \n",
              "\n",
              " pool3_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " drop3_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " conv4_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> \n",
              "\n",
              " batch_normalization_9                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                               \n",
              "\n",
              " conv4_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> \n",
              "\n",
              " batch_normalization_10                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                               \n",
              "\n",
              " conv4_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> \n",
              "\n",
              " batch_normalization_11                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                               \n",
              "\n",
              " conv4_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> \n",
              "\n",
              " batch_normalization_12                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                               \n",
              "\n",
              " pool4_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " drop4_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " conv5_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> \n",
              "\n",
              " batch_normalization_13                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                               \n",
              "\n",
              " conv5_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> \n",
              "\n",
              " batch_normalization_14                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                               \n",
              "\n",
              " conv5_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> \n",
              "\n",
              " batch_normalization_15                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                               \n",
              "\n",
              " conv5_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> \n",
              "\n",
              " pool5_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " drop5_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,591</span> \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,111,367\u001b[0m (50.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,111,367</span> (50.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,103,431\u001b[0m (49.99 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,103,431</span> (49.99 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m7,936\u001b[0m (31.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,936</span> (31.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "history = model.fit(train_flow,\n",
        "                    epochs=num_epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=test_flow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiScdv8Tlhu0",
        "outputId": "1c985892-450f-4340-c100-0c073b00af5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m 27/139\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16:49\u001b[0m 9s/step - accuracy: 0.2167 - loss: 2.0976"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_json = model.to_json()\n",
        "# with open(\"model.json\", \"w\") as json_file:\n",
        "#     json_file.write(model_json)\n",
        "# model.save_weights(\"model.h5\")\n",
        "# print(\"Saved model to disk\")\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "model.save_weights(\"model.weights.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "metadata": {
        "id": "LayDSNxrlk4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#joblib.dump(model, \"clf.pkl\")"
      ],
      "metadata": {
        "id": "lah-I4aalv5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.models import model_from_json\n",
        "# model = model_from_json(open(\"model.json\", \"r\").read())\n",
        "# model.load_weights('model.h5')\n",
        "\n",
        "from tensorflow.keras.models import model_from_json\n",
        "model = model_from_json(open(\"model.json\", \"r\").read())\n",
        "model.load_weights(\"model.weights.h5\")  # Update to match the saved filename"
      ],
      "metadata": {
        "id": "Sjv6XFqalx4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Try different camera indices\n",
        "for i in range(5):\n",
        "    cap = cv2.VideoCapture(i)\n",
        "    if cap.isOpened():\n",
        "        print(f\"Camera opened successfully at index {i}.\")\n",
        "        cap.release()  # Release the camera after testing\n",
        "        break\n",
        "    else:\n",
        "        print(f\"Failed to open camera at index {i}.\")\n",
        "\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "--o_LN0ql1Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import cv2\n",
        "# face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "face_haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
      ],
      "metadata": {
        "id": "z0efEq1wl5rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cap=cv2.VideoCapture(1)\n",
        "while cap.isOpened():\n",
        "    res,frame=cap.read()\n",
        "    height, width , channel = frame.shape\n",
        "    # Creating an Overlay window to write prediction and cofidence\n",
        "    sub_img = frame[0:int(height/6),0:int(width)]\n",
        "    black_rect = np.ones(sub_img.shape, dtype=np.uint8)*0\n",
        "    res = cv2.addWeighted(sub_img, 0.77, black_rect,0.23, 0)\n",
        "    FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    FONT_SCALE = 0.8\n",
        "    FONT_THICKNESS = 2\n",
        "    lable_color = (10, 10, 255)\n",
        "    lable = \"Emotion Detection\"\n",
        "    lable_dimension = cv2.getTextSize(lable,FONT ,FONT_SCALE,FONT_THICKNESS)[0]\n",
        "    textX = int((res.shape[1] - lable_dimension[0]) / 2)\n",
        "    textY = int((res.shape[0] + lable_dimension[1]) / 2)\n",
        "    cv2.putText(res, lable, (textX,textY), FONT, FONT_SCALE, (0,0,0), FONT_THICKNESS)# prediction part --------------------------------------------------------------------------\n",
        "    gray_image= cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_haar_cascade.detectMultiScale(gray_image )\n",
        "    try:\n",
        "        for (x,y, w, h) in faces:\n",
        "            cv2.rectangle(frame, pt1 = (x,y),pt2 = (x+w, y+h), color = (255,0,0),thickness =  2)\n",
        "            roi_gray = gray_image[y-5:y+h+5,x-5:x+w+5]\n",
        "            roi_gray=cv2.resize(roi_gray,(48,48))\n",
        "            image_pixels = img_to_array(roi_gray)\n",
        "            image_pixels = np.expand_dims(image_pixels, axis = 0)\n",
        "            image_pixels /= 255\n",
        "            predictions = model.predict(image_pixels)\n",
        "            max_index = np.argmax(predictions[0])\n",
        "            emotion_detection = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
        "            emotion_prediction = emotion_detection[max_index]\n",
        "            cv2.putText(res, \"Sentiment: {}\".format(emotion_prediction), (0,textY+22+5), FONT,0.7, lable_color,2)\n",
        "            lable_violation = 'Confidence: {}'.format(str(np.round(np.max(predictions[0])*100,1))+ \"%\")\n",
        "            violation_text_dimension = cv2.getTextSize(lable_violation,FONT,FONT_SCALE,FONT_THICKNESS )[0]\n",
        "            violation_x_axis = int(res.shape[1]- violation_text_dimension[0])\n",
        "            cv2.putText(res, lable_violation, (violation_x_axis,textY+22+5), FONT,0.7, lable_color,2)\n",
        "    except :\n",
        "        pass\n",
        "    frame[0:int(height/6),0:int(width)] = res\n",
        "    cv2.imshow('frame', frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        breakcap.release()\n",
        "cv2.destroyAllWindows"
      ],
      "metadata": {
        "id": "KKo0SORPl8Jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(2)"
      ],
      "metadata": {
        "id": "pu7MDoYImDBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Try different camera indices\n",
        "for i in range(5):  # Adjust the range as needed\n",
        "    cap = cv2.VideoCapture(i)\n",
        "    if cap.isOpened():\n",
        "        print(f\"Camera opened successfully at index {i}.\")\n",
        "        # Optional: Show a frame to confirm it's working\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            cv2.imshow(f'Camera {i}', frame)\n",
        "            cv2.waitKey(1000)  # Display for 1 second\n",
        "        cap.release()  # Release the camera after testing\n",
        "        break\n",
        "    else:\n",
        "        print(f\"Failed to open camera at index {i}.\")\n",
        "\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "KS_3gqJbmE4s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}